import boto3
import pandas as pd
import io

s3 = boto3.client('s3')

bucket = "your-bucket-name"
prefix = "your/prefix/path/"  # 폴더 경로 끝에 '/' 포함

# 1. parquet 파일 리스트 수집
response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.parquet')]

# 2. 각 parquet 파일 읽어서 병합
dfs = []
for key in files:
    obj = s3.get_object(Bucket=bucket, Key=key)
    df_part = pd.read_parquet(io.BytesIO(obj['Body'].read()))
    dfs.append(df_part)

# 3. 하나의 DataFrame으로 통합
df = pd.concat(dfs, ignore_index=True)

# 4. timestamp 오류 방지 처리
df['timestamp_col'] = pd.to_datetime(df['timestamp_col'], errors='coerce')











import awswrangler as wr

# Glue 카탈로그에서 S3 경로를 확인하거나 직접 경로 입력
s3_path = "s3://your-bucket/your-folder/"  # 예: s3://my-bucket/data/year=2025/month=06/day=18/

# S3 경로에 있는 모든 Parquet 파일을 하나의 DataFrame으로 읽기
df = wr.s3.read_parquet(path=s3_path, dataset=True)

# 오류 방지를 위한 timestamp 변환
df['timestamp_col'] = pd.to_datetime(df['timestamp_col'], errors='coerce')



import boto3
import io
import pandas as pd

s3 = boto3.client('s3')

# 1. 입력 정보
bucket = 'your-bucket'
input_key = 'raw/schema=mine/table=maole/year=2025/month=06/day=18/yourfile.parquet'

# 2. 내가 지정할 새 경로 앞부분
custom_prefix = 'processed-data'

# 3. table=부터 추출
split_parts = input_key.split('/')

# 'table='이 시작되는 인덱스 찾기
table_index = next(i for i, part in enumerate(split_parts) if part.startswith('table='))

# 'table=...'부터 끝까지 이어붙이기
partition_and_filename = '/'.join(split_parts[table_index:])

# 4. 최종 output_key
output_key = f"{custom_prefix}/{partition_and_filename}"

# 5. 읽고 처리
response = s3.get_object(Bucket=bucket, Key=input_key)
df = pd.read_parquet(response['Body'])
df_clean = df.dropna()

# 6. 저장
out_buffer = io.BytesIO()
df_clean.to_parquet(out_buffer, index=False)
s3.put_object(Bucket=bucket, Key=output_key, Body=out_buffer.getvalue())

print(f"✅ 저장 완료: s3://{bucket}/{output_key}")


---



import pandas as pd
import boto3
import io
import os

s3 = boto3.client('s3')

# 1. 원본 파일 경로
bucket = ''
input_key = ''

# 2. 원본 파일명 추출
original_filename = os.path.basename(input_key) 
base_name, ext = os.path.splitext(original_filename)  

# 3. 데이터 로드
response = s3.get_object(Bucket=bucket, Key=input_key)
df = pd.read_parquet(response['Body'])

# 4. 전처리
df_clean = 

# 5. 파티션 컬럼 추출
year = str(df_clean.iloc[0]['year'])
month = str(df_clean.iloc[0]['month']).zfill(2)
day = str(df_clean.iloc[0]['day']).zfill(2)

# ✅ 6. 새 파일명 구성: 기존 이름 + _processed
new_filename = f"{base_name}_processed{ext}"

# ✅ 7. 저장 경로
output_key = f"processed/year={year}/month={month}/day={day}/{new_filename}"

# 8. 저장
out_buffer = io.BytesIO()
df_clean.to_parquet(out_buffer, index=False)
s3.put_object(Bucket=bucket, Key=output_key, Body=out_buffer.getvalue())




cdc lambda
import boto3
import urllib.parse

def lambda_handler(event, context):
    glue = boto3.client('glue')
    
    # S3 이벤트에서 파일 경로 추출
    record = event['Records'][0]
    bucket = record['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(record['s3']['object']['key'])

    print(f"Triggered by file: s3://{bucket}/{key}")
    
    # Glue Job 파라미터 전달
    response = glue.start_job_run(
        JobName='your-job-name',
        Arguments={
            '--s3_input_path': f's3://{bucket}/{key}'
        }
    )

    print("✅ Started Glue Job:", response['JobRunId'])
    return {
        'status': 'started',
        'job_run_id': response['JobRunId'],
        'input_file': key
    }

cdc job
import sys

# Glue Job Arguments에서 s3_input_path 가져오기
from awsglue.utils import getResolvedOptions
args = getResolvedOptions(sys.argv, ['s3_input_path'])
input_path = args['s3_input_path']

print(f"Input path from Lambda: {input_path}")



